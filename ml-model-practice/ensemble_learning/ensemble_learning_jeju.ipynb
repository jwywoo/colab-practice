{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble Learning\n",
        "- Getting preprocessed correlated data from Google Drive\n",
        "- Using KNN, SVM, Logistic Regression and Ridge Regression for base modesl\n",
        "- Using Bagging"
      ],
      "metadata": {
        "id": "AK9aUcRoh5Mi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import and Setting"
      ],
      "metadata": {
        "id": "64-CszvqmW1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/content/drive')\n",
        "import shutil\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOMOH0_Sh4v5",
        "outputId": "e58738c6-2f28-4718-d1a1-ca3afa92ae65"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Adding Korean"
      ],
      "metadata": {
        "id": "jHwsqilUh4J4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0O2iyvg2g9Ru"
      },
      "outputs": [],
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "import unicodedata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update -qq\n",
        "!apt-get install fonts-nanum* -qq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8eFsSOumimo",
        "outputId": "36388ad0-dac0-429e-9475-ceca014360be"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fe = fm.FontEntry(fname=r'/usr/share/fonts/truetype/nanum/NanumGothic.ttf', name='NanumGothic') #파일 저장되어있는 경로와 이름 설정\n",
        "fm.fontManager.ttflist.insert(0, fe)\n",
        "plt.rcParams.update({'font.size': 8, 'font.family': 'NanumGothic'})\n",
        "mpl.rcParams['axes.unicode_minus'] = False"
      ],
      "metadata": {
        "id": "CzgKIPhRmljL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Preprocessed Data\n"
      ],
      "metadata": {
        "id": "-rbtiG6Lmibj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting CSV\n",
        "directory_path = '/content/drive/MyDrive/CSVs/JejuWeather'\n",
        "csv_files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
        "\n",
        "# Getting paths to obs directory\n",
        "obs_dirs_paths = []\n",
        "for obs in csv_files:\n",
        "  obs_korean = obs.replace('.csv', '')\n",
        "  obs_dirs_paths.append(os.path.join(directory_path, obs_korean))\n",
        "\n",
        "# Getting paths to relavance csv of each obs directory\n",
        "related_columns_csvs = []\n",
        "for obs in obs_dirs_paths:\n",
        "  temp = []\n",
        "  temp.append(obs)\n",
        "  temp.append(os.path.join(obs,[f for f in os.listdir(obs) if f.startswith('relevance_')][0]))\n",
        "  related_columns_csvs.append(temp)"
      ],
      "metadata": {
        "id": "fIdEW2Pfm1u4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common function & Imports"
      ],
      "metadata": {
        "id": "Hoqndy14IxVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import joblib"
      ],
      "metadata": {
        "id": "0eAFM9nVDXEB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def three_days(obs_csv_relevance):\n",
        "  data = pd.read_csv(obs_csv_relevance).sort_values('baseDate')\n",
        "  prev_day_1 = data.shift(1).drop(columns=['baseDate'])\n",
        "  prev_day_2 = data.shift(2).drop(columns=['baseDate'])\n",
        "  prev_day_3 = data.shift(3).drop(columns=['baseDate'])\n",
        "\n",
        "\n",
        "  combined_data = pd.concat([data['baseDate'], data['dailyRainfall'],prev_day_1.add_suffix('_prev1'), prev_day_2.add_suffix('_prev2'), prev_day_3.add_suffix('_prev3')], axis=1)\n",
        "  combined_data = combined_data.dropna()\n",
        "  combined_data = combined_data.reset_index(drop=True)\n",
        "\n",
        "  return combined_data"
      ],
      "metadata": {
        "id": "7dgcEmg3u0D8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def x_y_split(obs_csv_relevance):\n",
        "  data = three_days(obs_csv_relevance)\n",
        "\n",
        "  # X: related features\n",
        "  X_columns = data.drop(columns=['baseDate', 'dailyRainfall'])\n",
        "\n",
        "  # y: target\n",
        "  y = (data['dailyRainfall'] > 0).astype(int)\n",
        "  y = y.reset_index(drop=True)\n",
        "\n",
        "  return X_columns, y"
      ],
      "metadata": {
        "id": "B-MvvpjyIxHT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base Models: Linear Regression"
      ],
      "metadata": {
        "id": "pP1uzyHrrbkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "KLQOzPTIKFdw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jeju_weather_linear(obs_csv_relevance):\n",
        "  # Splitting\n",
        "  X, y = x_y_split(obs_csv_relevance)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "  X_train_scaled = StandardScaler().fit_transform(X_train)\n",
        "  X_test_scaled = StandardScaler().fit_transform(X_test)\n",
        "  # Model & Training\n",
        "  logistic_reg = LogisticRegression(max_iter=1000)\n",
        "  logistic_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "  # Prediction\n",
        "  accuracy = logistic_reg.score(X_test_scaled, y_test)\n",
        "  print(\"Lienar Regression Accuracy:\", accuracy)\n",
        "  return logistic_reg"
      ],
      "metadata": {
        "id": "ItpRbevBrf0g"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN"
      ],
      "metadata": {
        "id": "5fdxHhCuC9n3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "metadata": {
        "id": "GQ3cF7f6DL5Z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jeju_weather_knn(obs_csv_relevance):\n",
        "  # Splitting\n",
        "  X, y = x_y_split(obs_csv_relevance)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "  X_train_scaled = StandardScaler().fit_transform(X_train)\n",
        "  X_test_scaled = StandardScaler().fit_transform(X_test)\n",
        "\n",
        "  knn = KNeighborsClassifier(n_neighbors=5)\n",
        "  knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "  accuracy = knn.score(X_test_scaled, y_test)\n",
        "  print(\"KNN Accuracy:\", accuracy)\n",
        "  return knn"
      ],
      "metadata": {
        "id": "LIuR63c1C_Ct"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support Vector Machine"
      ],
      "metadata": {
        "id": "JHYPi0siJifP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "JEZmfNWYyGPv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jeju_weather_svm(obs_csv_relevance):\n",
        "  X,y = x_y_split(obs_csv_relevance)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "  X_train_scaled = StandardScaler().fit_transform(X_train)\n",
        "  X_test_scaled = StandardScaler().fit_transform(X_test)\n",
        "\n",
        "  svm = SVC(kernel='linear')\n",
        "  svm.fit(X_train_scaled, y_train)\n",
        "\n",
        "  accuracy = svm.score(X_test_scaled, y_test)\n",
        "  print(\"SVM Accuracy:\", accuracy)\n",
        "  return svm"
      ],
      "metadata": {
        "id": "5oYTILQcJiGy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ridge Regression Model"
      ],
      "metadata": {
        "id": "-HQgvQRs9wbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "v_HaRYlE-QQH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jeju_weather_ridge(obs_csv_relevance):\n",
        "  X,y = x_y_split(obs_csv_relevance)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "  X_train_scaled = StandardScaler().fit_transform(X_train)\n",
        "  X_test_scaled = StandardScaler().fit_transform(X_test)\n",
        "\n",
        "  ridge = Ridge(alpha=1.0)\n",
        "  ridge.fit(X_train_scaled, y_train)\n",
        "\n",
        "  y_pred = ridge.predict(X_test_scaled)\n",
        "\n",
        "  mse = mean_squared_error(y_test, y_pred)\n",
        "  r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "  print(\"Mean Squared Error:\", mse)\n",
        "  print(\"R^2 Score:\", r2)\n",
        "  return ridge"
      ],
      "metadata": {
        "id": "icl0VQtu9w-l"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble Learning"
      ],
      "metadata": {
        "id": "kqQLB3hP18DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier"
      ],
      "metadata": {
        "id": "ayvwBWtx17ac"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jeju_weather_ensemble(log_reg, knn, svm, obs_csv_relevance):\n",
        "  X,y = x_y_split(obs_csv_relevance)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "  X_train_scaled = StandardScaler().fit_transform(X_train)\n",
        "  X_test_scaled = StandardScaler().fit_transform(X_test)\n",
        "\n",
        "  ensemble_model = VotingClassifier(estimators=[\n",
        "      ('log_reg', log_reg),\n",
        "      # ('knn', knn),\n",
        "      ('svm', svm)], voting='hard')\n",
        "  ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = ensemble_model.predict(X_test)\n",
        "  accuracy = ensemble_model.score(X_test_scaled, y_test)\n",
        "  print(\"Ensemble Accuracy:\", accuracy)\n",
        "  return ensemble_model"
      ],
      "metadata": {
        "id": "QDszOOYC265b"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "vldElfl0Hfq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for related_columns_csv in related_columns_csvs:\n",
        "  obs_dir_path = related_columns_csv[0]\n",
        "  korean_name = obs_dir_path.split(\"/\")[-1]\n",
        "  print(korean_name)\n",
        "  current_log_reg_model = jeju_weather_linear(related_columns_csv[1])\n",
        "  current_knn_model = jeju_weather_knn(related_columns_csv[1])\n",
        "  current_ridge_model = jeju_weather_ridge(related_columns_csv[1])\n",
        "  current_svm_model = jeju_weather_svm(related_columns_csv[1])\n",
        "  ensemble = jeju_weather_ensemble(current_log_reg_model, current_knn_model, current_svm_model, related_columns_csv[1])\n",
        "  if os.path.exists(os.path.join(obs_dir_path, korean_name+'_esm_model.pkl')):\n",
        "     os.remove(os.path.join(obs_dir_path, korean_name+'_esm_model.pkl'))\n",
        "  joblib.dump(current_log_reg_model, os.path.join(obs_dir_path, korean_name+'_esm_model.pkl'))\n",
        "  print(korean_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1355TJYZHe30",
        "outputId": "da39b554-552d-45fd-c7e1-242363503f70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "마라도\n",
            "Lienar Regression Accuracy: 0.6897001303780965\n",
            "KNN Accuracy: 0.6649282920469362\n",
            "Mean Squared Error: 0.2088932868891197\n",
            "R^2 Score: 0.04523569790918203\n",
            "SVM Accuracy: 0.6883963494132985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1gS0IVvDxWPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KMeans Cluster: Exclude"
      ],
      "metadata": {
        "id": "uiziVDd80s1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "AhOLj0PE1C5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_elbow_point(wcss):\n",
        "    deltas = np.diff(wcss)\n",
        "    second_deltas = np.diff(deltas)\n",
        "    elbow_point = np.argwhere(second_deltas > 0)[0][0] + 2\n",
        "    return elbow_point"
      ],
      "metadata": {
        "id": "COWwVZuo7SoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jeju_weather_kmeans(obs_csv_relevance):\n",
        "  # Features and scaling\n",
        "  three_days_combined = three_days(obs_csv_relevance)\n",
        "  features = three_days_combined.drop(columns=['baseDate', 'dailyRainfall'])\n",
        "  scaler = StandardScaler()\n",
        "  scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "  # getting optimal nums of cluters\n",
        "  wcss = []\n",
        "  max_clusters = 10\n",
        "  for i in range(1, max_clusters + 1):\n",
        "    kmeans = KMeans(n_clusters=i, n_init=10, random_state=42)\n",
        "    kmeans.fit(scaled_features)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "  optimal_k = find_elbow_point(wcss)\n",
        "  # Model & Training\n",
        "  kmeans = KMeans(n_clusters=optimal_k, n_init=10, random_state=42)\n",
        "  three_days_combined['cluster'] = kmeans.fit_predict(scaled_features)\n",
        "  print(three_days_combined['cluster'].value_counts())\n"
      ],
      "metadata": {
        "id": "imsUF45B01bi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}