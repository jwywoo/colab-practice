{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK9aUcRoh5Mi"
      },
      "source": [
        "## Ensemble Learning\n",
        "- Getting preprocessed correlated data from Google Drive\n",
        "- Using KNN, SVM, Logistic Regression and Ridge Regression for base modesl\n",
        "- Using Bagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64-CszvqmW1T"
      },
      "source": [
        "## Import and Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOMOH0_Sh4v5",
        "outputId": "3ec0c94f-578a-4205-d1a1-94a6c699c198"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/content/drive')\n",
        "import shutil\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHwsqilUh4J4"
      },
      "source": [
        "\n",
        "# Adding Korean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0O2iyvg2g9Ru"
      },
      "outputs": [],
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "import unicodedata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8eFsSOumimo",
        "outputId": "a8e879ed-1dd1-4d9e-b21b-efef76415c62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 123586 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20200506-1_all.deb ...\n",
            "Unpacking fonts-nanum (20200506-1) ...\n",
            "Selecting previously unselected package fonts-nanum-coding.\n",
            "Preparing to unpack .../fonts-nanum-coding_2.5-3_all.deb ...\n",
            "Unpacking fonts-nanum-coding (2.5-3) ...\n",
            "Selecting previously unselected package fonts-nanum-eco.\n",
            "Preparing to unpack .../fonts-nanum-eco_1.000-7_all.deb ...\n",
            "Unpacking fonts-nanum-eco (1.000-7) ...\n",
            "Selecting previously unselected package fonts-nanum-extra.\n",
            "Preparing to unpack .../fonts-nanum-extra_20200506-1_all.deb ...\n",
            "Unpacking fonts-nanum-extra (20200506-1) ...\n",
            "Setting up fonts-nanum-extra (20200506-1) ...\n",
            "Setting up fonts-nanum (20200506-1) ...\n",
            "Setting up fonts-nanum-coding (2.5-3) ...\n",
            "Setting up fonts-nanum-eco (1.000-7) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get update -qq\n",
        "!apt-get install fonts-nanum* -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CzgKIPhRmljL"
      },
      "outputs": [],
      "source": [
        "fe = fm.FontEntry(fname=r'/usr/share/fonts/truetype/nanum/NanumGothic.ttf', name='NanumGothic') #파일 저장되어있는 경로와 이름 설정\n",
        "fm.fontManager.ttflist.insert(0, fe)\n",
        "plt.rcParams.update({'font.size': 8, 'font.family': 'NanumGothic'})\n",
        "mpl.rcParams['axes.unicode_minus'] = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rbtiG6Lmibj"
      },
      "source": [
        "## Getting Preprocessed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "fIdEW2Pfm1u4"
      },
      "outputs": [],
      "source": [
        "# Getting CSV\n",
        "directory_path = '/content/drive/MyDrive/CSVs/JejuWeather'\n",
        "csv_files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
        "\n",
        "# Getting paths to obs directory\n",
        "obs_dirs_paths = []\n",
        "for obs in csv_files:\n",
        "  obs_korean = obs.replace('.csv', '')\n",
        "  obs_dirs_paths.append(os.path.join(directory_path, obs_korean))\n",
        "\n",
        "# Getting paths to relavance csv of each obs directory\n",
        "related_columns_csvs = []\n",
        "for obs in obs_dirs_paths:\n",
        "  temp = []\n",
        "  temp.append(obs)\n",
        "  temp.append(os.path.join(obs,[f for f in os.listdir(obs) if f.startswith('relevance_')][0]))\n",
        "  related_columns_csvs.append(temp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hoqndy14IxVU"
      },
      "source": [
        "## Common function & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eAFM9nVDXEB"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "7dgcEmg3u0D8"
      },
      "outputs": [],
      "source": [
        "def three_days(obs_csv_relevance):\n",
        "  data = pd.read_csv(obs_csv_relevance).sort_values('baseDate')\n",
        "  prev_day_1 = data.shift(1).drop(columns=['baseDate'])\n",
        "  prev_day_2 = data.shift(2).drop(columns=['baseDate'])\n",
        "  prev_day_3 = data.shift(3).drop(columns=['baseDate'])\n",
        "\n",
        "\n",
        "  combined_data = pd.concat([data['baseDate'], data['dailyRainfall'],prev_day_1.add_suffix('_prev1'), prev_day_2.add_suffix('_prev2'), prev_day_3.add_suffix('_prev3')], axis=1)\n",
        "  combined_data = combined_data.dropna()\n",
        "  combined_data = combined_data.reset_index(drop=True)\n",
        "\n",
        "  return combined_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "B-MvvpjyIxHT"
      },
      "outputs": [],
      "source": [
        "def x_y_split(obs_csv_relevance):\n",
        "  data = three_days(obs_csv_relevance)\n",
        "\n",
        "  # X: related features\n",
        "  X_columns = data.drop(columns=['baseDate', 'dailyRainfall'])\n",
        "\n",
        "  # y: target\n",
        "  y = (data['dailyRainfall'] > 0).astype(int)\n",
        "  y = y.reset_index(drop=True)\n",
        "\n",
        "  return X_columns, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP1uzyHrrbkX"
      },
      "source": [
        "## Base Models: Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "KLQOzPTIKFdw"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "ItpRbevBrf0g"
      },
      "outputs": [],
      "source": [
        "def jeju_weather_linear(obs_csv_relevance):\n",
        "  # Splitting\n",
        "  X, y = x_y_split(obs_csv_relevance)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "  X_train_scaled = StandardScaler().fit_transform(X_train)\n",
        "  X_test_scaled = StandardScaler().fit_transform(X_test)\n",
        "  # Model & Training\n",
        "  logistic_reg = LogisticRegression(max_iter=1000)\n",
        "  logistic_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "  # Prediction\n",
        "  accuracy = logistic_reg.score(X_test_scaled, y_test)\n",
        "  print(\"Lienar Regression Accuracy:\", accuracy)\n",
        "  return logistic_reg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fdxHhCuC9n3"
      },
      "source": [
        "## KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "GQ3cF7f6DL5Z"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "LIuR63c1C_Ct"
      },
      "outputs": [],
      "source": [
        "def jeju_weather_knn(obs_csv_relevance):\n",
        "  # Splitting\n",
        "  X, y = x_y_split(obs_csv_relevance)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "  X_train_scaled = StandardScaler().fit_transform(X_train)\n",
        "  X_test_scaled = StandardScaler().fit_transform(X_test)\n",
        "\n",
        "  knn = KNeighborsClassifier(n_neighbors=5)\n",
        "  knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "  accuracy = knn.score(X_test_scaled, y_test)\n",
        "  print(\"KNN Accuracy:\", accuracy)\n",
        "  return knn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHYPi0siJifP"
      },
      "source": [
        "## Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oYTILQcJiGy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiziVDd80s1I"
      },
      "source": [
        "## KMeans Cluster: Exclude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "AhOLj0PE1C5o"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "COWwVZuo7SoB"
      },
      "outputs": [],
      "source": [
        "def find_elbow_point(wcss):\n",
        "    deltas = np.diff(wcss)\n",
        "    second_deltas = np.diff(deltas)\n",
        "    elbow_point = np.argwhere(second_deltas > 0)[0][0] + 2\n",
        "    return elbow_point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "imsUF45B01bi"
      },
      "outputs": [],
      "source": [
        "def jeju_weather_kmeans(obs_csv_relevance):\n",
        "  # Features and scaling\n",
        "  three_days_combined = three_days(obs_csv_relevance)\n",
        "  features = three_days_combined.drop(columns=['baseDate', 'dailyRainfall'])\n",
        "  scaler = StandardScaler()\n",
        "  scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "  # getting optimal nums of cluters\n",
        "  wcss = []\n",
        "  max_clusters = 10\n",
        "  for i in range(1, max_clusters + 1):\n",
        "    kmeans = KMeans(n_clusters=i, n_init=10, random_state=42)\n",
        "    kmeans.fit(scaled_features)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "  optimal_k = find_elbow_point(wcss)\n",
        "  # Model & Training\n",
        "  kmeans = KMeans(n_clusters=optimal_k, n_init=10, random_state=42)\n",
        "  three_days_combined['cluster'] = kmeans.fit_predict(scaled_features)\n",
        "  print(three_days_combined['cluster'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vldElfl0Hfq7"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1355TJYZHe30",
        "outputId": "6f443b82-5946-4e9d-d4d3-1f84ca1215fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "마라도\n",
            "Lienar Regression Accuracy: 0.6897001303780965\n",
            "KNN Accuracy: 0.6649282920469362\n"
          ]
        }
      ],
      "source": [
        "for related_columns_csv in related_columns_csvs:\n",
        "  obs_dir_path = related_columns_csv[0]\n",
        "  print(obs_dir_path.split(\"/\")[-1])\n",
        "  current_log_reg_model = jeju_weather_linear(related_columns_csv[1])\n",
        "  current_log_reg_model = jeju_weather_knn(related_columns_csv[1])\n",
        "  if os.path.exists(os.path.join(obs_dir_path, 'linear_reg_model.pkl')):\n",
        "    os.remove(os.path.join(obs_dir_path, 'linear_reg_model.pkl'))\n",
        "  joblib.dump(current_log_reg_model, os.path.join(obs_dir_path, 'linear_reg_model.pkl'))\n",
        "\n",
        "  break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
