{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Daily Weather Forcast from Jeju Datahub\n",
        "1. Daily weather forecast starting from 2014/01/01 ~ 2024/06/30\n",
        "  1. Python -> request and date\n",
        "  ``https://open.jejudatahub.net/api/proxy/1aD5taat1attaa51Db1511b51ab9Da19/{your_projectKey}?{params(key=value)}``\n",
        "2. Store the data into `jejuIsland.csv`\n",
        "3. Preprocess the data based on `observatoryName` and create a csv file based on the name.\n",
        "  1. Each csv file should have 10 years of daily weather forecast of `observatoryName`\n",
        "  2. Then replace missing values with neighbor values\n",
        "\n"
      ],
      "metadata": {
        "id": "NS4-PnAowK5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation, setting and import"
      ],
      "metadata": {
        "id": "RQXMlNlEKKro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation & Setting\n",
        "- Google Drive directory setting"
      ],
      "metadata": {
        "id": "URrPLwufLOYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "\n",
        "if (not os.path.exists('/content/drive/MyDrive/CSVs')):\n",
        "  os.makedirs('/content/drive/MyDrive/CSVs')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkCh-H-2NiDV",
        "outputId": "16089f1e-90d0-4fef-d880-b6b1c581cdbd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import\n",
        "- requests\n",
        "- datetime\n",
        "- time\n",
        "- google drive\n",
        "- shutil"
      ],
      "metadata": {
        "id": "qCpFC87bKd8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# requests\n",
        "import requests\n",
        "# datetime\n",
        "from datetime import datetime, timedelta\n",
        "# time - pause crawling for 0.5 sec\n",
        "import time\n",
        "# Userdata for secret key\n",
        "from google.colab import userdata\n",
        "# pandas\n",
        "import pandas as pd\n",
        "# moving files between dirs\n",
        "import shutil\n",
        "# numpy\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "dCdx3YlyKIPZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing functions\n",
        "- `dates_list_generation()`\n",
        "- `json_parsing_200()`\n",
        "- `duplicate_eraser()`\n",
        "- `missing_value_handling()`"
      ],
      "metadata": {
        "id": "u921-99BKRmV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ITMv1jlkwDXI"
      },
      "outputs": [],
      "source": [
        "## dates_list_generation\n",
        "def dates_list_generation(start_date, end_date):\n",
        "  dates = []\n",
        "  current = start_date\n",
        "  while current <= end_date:\n",
        "    dates.append(current.strftime('%Y%m%d'))\n",
        "    current += timedelta(days=1)\n",
        "  return dates"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Json -> CSV\n",
        "def json_parsing_200(data, file_path):\n",
        "  for i, record in enumerate(data['data']):\n",
        "    ob_name = record.get('observatoryName')\n",
        "    processed_record = {\n",
        "        'baseDate': record.get('baseDate'),\n",
        "        'averageTemperature': record.get('averageTemperature'),\n",
        "        'lowestTemperature': record.get('lowestTemperature'),\n",
        "        'lowestTemperatureTime': record.get('lowestTemperatureTime'),\n",
        "        'highestTemperature': record.get('highestTemperature'),\n",
        "        'highestTemperatureTime': record.get('highestTemperatureTime'),\n",
        "        'dailyRainfall': record.get('dailyRainfall'),\n",
        "        'maximumWindSpeed': record.get('maximumWindSpeed'),\n",
        "        'maximumWindSpeedTime': record.get('maximumWindSpeedTime'),\n",
        "        'averageWindSpeed': record.get('averageWindSpeed'),\n",
        "        'maximumWindSpeedDirection': record.get('maximumWindSpeedDirection')\n",
        "        }\n",
        "\n",
        "    new_row = pd.DataFrame([record])\n",
        "\n",
        "    # Google Drive folder\n",
        "    # Jeju_Weahter/{observatoryName}.csv\n",
        "    new_file_path = file_path + '/' + ob_name + '.csv'\n",
        "    if os.path.exists(new_file_path):\n",
        "      df = pd.read_csv(new_file_path)\n",
        "      df = pd.concat([df, new_row], ignore_index= True)\n",
        "      df.to_csv(new_file_path, index=False)\n",
        "    else:\n",
        "      new_row.to_csv(new_file_path, index=False)"
      ],
      "metadata": {
        "id": "ls8cwZC0yV66"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def missing_row_500(baseDate, observatoryName):\n",
        "  empty_row = {\n",
        "      'observatoryName': observatoryName,\n",
        "        'baseDate': int(baseDate),\n",
        "        'averageTemperature': np.nan,\n",
        "        'lowestTemperature': np.nan,\n",
        "        'lowestTemperatureTime': np.nan,\n",
        "        'highestTemperature': np.nan,\n",
        "        'highestTemperatureTime': np.nan,\n",
        "        'dailyRainfall': np.nan,\n",
        "        'maximumWindSpeed': np.nan,\n",
        "        'maximumWindSpeedTime': np.nan,\n",
        "        'averageWindSpeed': np.nan,\n",
        "        'maximumWindSpeedDirection': np.nan\n",
        "        }\n",
        "  return pd.DataFrame([empty_row])"
      ],
      "metadata": {
        "id": "hZP2hHFlAjKt"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## duplicate_eraser()\n",
        "def duplicate_eraser(df):\n",
        "  df_no_duplicates = df.drop_duplicates(subset='baseDate')\n",
        "  return df_no_duplicates"
      ],
      "metadata": {
        "id": "Lbmzs57caWxe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## missing value handling\n",
        "def missing_value_handling(df):\n",
        "  df = df.fillna(method='bfill')\n",
        "  df = df.fillna(method='ffill')\n",
        "  return df"
      ],
      "metadata": {
        "id": "nxDjPTRo0TT_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execution\n",
        "1. Generating dates: 10years\n",
        "2. Sending requests & Changing format\n",
        "  - response -> json\n",
        "  - json -> csv\n",
        "3. Saving into Google Drive\n",
        "4. Remove duplicated values -> based on baseDate\n",
        "5.  Data validity check\n",
        "  - Data that's not been update for a year cannot be used due to accuracy\n",
        "  - CSV file that has not been updated for 7 days can't be used\n",
        "  - CSV file that has less than 1000 rows won't be used\n",
        "    - It will be moved to not `Not_Valid` directory\n",
        "6. Missing Value\n",
        "  - Backward fill: `df.fillna(method='bfill')`\n",
        "  - Forward fill: `df.fillna(method='ffill')`"
      ],
      "metadata": {
        "id": "MgkupkMWKucJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1, 2, 3\n",
        "start_date = datetime(2024,1,1)\n",
        "end_date = datetime(2024,7,2)\n",
        "dates = dates_list_generation(start_date, end_date)\n",
        "file_path = '/content/drive/MyDrive/CSVs/JejuWeather'\n",
        "if (not os.path.exists(file_path)):\n",
        "  os.makedirs(file_path)\n",
        "start_time = time.time()\n",
        "for year_month_day in dates:\n",
        "  project_key = userdata.get('jejuWeatherKey')\n",
        "  url = f'https://open.jejudatahub.net/api/proxy/1aD5taat1attaa51Db1511b51ab9Da19/{project_key}?searchDate={year_month_day}'\n",
        "  time.sleep(1)\n",
        "  response = requests.get(url)\n",
        "  if response.status_code == 200:\n",
        "    try:\n",
        "      print(f\"Status 200 JSON for {year_month_day}\")\n",
        "      response_json = response.json()\n",
        "      json_parsing_200(response_json, file_path)\n",
        "    except ValueError as e:\n",
        "      print(f\"Error decoding JSON for {year_month_day}: {e}\")\n",
        "      print(response.text)\n",
        "  else:\n",
        "    print(f\"Request failed for {year_month_day} with status code: {response.status_code}\")\n",
        "    print(response.text)\n",
        "\n",
        "current_time = time.time()\n",
        "print(f\"Elapsed time: {current_time - start_time} seconds\")"
      ],
      "metadata": {
        "id": "v3IzqLfyJQ1n",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 4 Erasing duplicated values\n",
        "# getting list of csv files\n",
        "directory_path = '/content/drive/MyDrive/CSVs/JejuWeather'\n",
        "\n",
        "if (not os.path.exists(directory_path)):\n",
        "  print('Directory not exist')\n",
        "  exit()\n",
        "\n",
        "csv_files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
        "\n",
        "# # Test\n",
        "# test_df = pd.read_csv(os.path.join(directory_path, '강정.csv'))\n",
        "# print(test_df.size)\n",
        "# test_df = duplicate_eraser(test_df)\n",
        "# print(test_df.size)\n",
        "for csv_file in csv_files:\n",
        "  file_path = os.path.join(directory_path, csv_file)\n",
        "  df = pd.read_csv(file_path)\n",
        "  print(csv_file)\n",
        "  print(df.size)\n",
        "  no_dup_df = duplicate_eraser(df)\n",
        "  print(no_dup_df.size)\n",
        "  no_dup_df.to_csv(file_path, index=False)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "aywE8zsFh17o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5 Data validity check\n",
        "csv_files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
        "not_valid_dir = '/content/drive/MyDrive/CSVs/JejuWeather/Not_Valid'\n",
        "if (not os.path.exists(not_valid_dir)):\n",
        "  os.makedirs(not_valid_dir)\n",
        "for csv_file in csv_files:\n",
        "  file_path = os.path.join(directory_path, csv_file)\n",
        "  df = pd.read_csv(file_path)\n",
        "  # last updated date check -> less then 7 days\n",
        "  first_date_str = str(df['baseDate'].iloc[0])\n",
        "  last_date_str = str(df['baseDate'].iloc[-1])\n",
        "  first_date = datetime.strptime(first_date_str, \"%Y%m%d\")\n",
        "  last_date = datetime.strptime(last_date_str,\"%Y%m%d\")\n",
        "  days_passed = (datetime.now() - last_date).days\n",
        "  total_days = (last_date - first_date).days\n",
        "  # # of rows check -> more than 10000\n",
        "  if (days_passed > 15 or total_days < 1000):\n",
        "    print(f\"{csv_file} is not valid\")\n",
        "    shutil.move(file_path, '/content/drive/MyDrive/CSVs/JejuWeather/Not_Valid/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM_uCXL9pVmg",
        "outputId": "47fb1e62-9997-4aee-8757-15e0a4a5fe0e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "강정.csv is not valid\n",
            "한림.csv is not valid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 6 Missing Values & Rows\n",
        "# Filling empty rows\n",
        "directory_path = '/content/drive/MyDrive/CSVs/JejuWeather'\n",
        "csv_files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
        "\n",
        "# Getting the last date among csv files\n",
        "last_date = int((datetime.now()-timedelta(days=7)).strftime('%Y%m%d'))\n",
        "for csv_file in csv_files:\n",
        "  file_path = os.path.join(directory_path, csv_file)\n",
        "  df = pd.read_csv(file_path)\n",
        "  current_last_date = df.tail(1)['baseDate'].values[0]\n",
        "  if (last_date < current_last_date):\n",
        "    last_date = current_last_date\n",
        "\n",
        "# filling missing rows\n",
        "for csv_file in csv_files:\n",
        "  file_path = os.path.join(directory_path, csv_file)\n",
        "  df = pd.read_csv(file_path)\n",
        "  current_date_of_current = datetime.strptime(str(df.head(1)['baseDate'].values[0]), \"%Y%m%d\")\n",
        "  last_date_of_current = datetime.strptime(str(last_date), \"%Y%m%d\")\n",
        "  print(csv_file)\n",
        "  counter = 0\n",
        "  new_df_to_append = pd.DataFrame()\n",
        "  observatoryName = csv_file.split('.')[0]\n",
        "  while (current_date_of_current <= last_date_of_current):\n",
        "    current_date_int = int(current_date_of_current.strftime('%Y%m%d'))\n",
        "    if (df[df['baseDate'] == current_date_int].empty):\n",
        "      new_df_to_append = pd.concat([new_df_to_append, missing_row_500(current_date_int, observatoryName)], ignore_index=True)\n",
        "    current_date_of_current += timedelta(days=1)\n",
        "  df = pd.concat([df,new_df_to_append], ignore_index=True)\n",
        "  df = duplicate_eraser(df)\n",
        "  df = df.sort_values(by='baseDate')\n",
        "  df.to_csv(file_path, index=False)\n",
        "\n",
        "# checking missing rows\n",
        "print(\"\\nMissing rows check\")\n",
        "for csv_file in csv_files:\n",
        "  file_path = os.path.join(directory_path, csv_file)\n",
        "  df = pd.read_csv(file_path)\n",
        "  current_date_of_current = datetime.strptime(str(df.head(1)['baseDate'].values[0]), \"%Y%m%d\")\n",
        "  last_date_of_current = datetime.strptime(str(df.head(1)['baseDate'].values[0]), \"%Y%m%d\")\n",
        "  counter = 0\n",
        "  while (current_date_of_current <= last_date_of_current):\n",
        "    current_date_int = int(current_date_of_current.strftime('%Y%m%d'))\n",
        "    if (df[df['baseDate'] == current_date_int].empty):\n",
        "      counter+=1\n",
        "    current_date_of_current += timedelta(days=1)\n",
        "  print(f\"{csv_file} has {counter} missing rows\")"
      ],
      "metadata": {
        "id": "Tj4eygsfnA3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 6 Missing Values & Rows\n",
        "# Filling missing values\n",
        "directory_path = '/content/drive/MyDrive/CSVs/JejuWeather'\n",
        "csv_files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
        "\n",
        "for csv_file in csv_files:\n",
        "  file_path = os.path.join(directory_path, csv_file)\n",
        "  df = pd.read_csv(file_path)\n",
        "  # choosing linear interpolation\n",
        "  print(csv_file)\n",
        "  df_filled = df.interpolate(method='linear')\n",
        "  print(\"Done\")\n",
        "  df_filled.to_csv(file_path, index=False)"
      ],
      "metadata": {
        "id": "AVc_Kif-17X8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}