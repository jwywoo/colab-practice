{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOieb2510SLuPG8OvHUD/pw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwywoo/colab-practice/blob/feat%5D-%231-dataAcqusition/Day1_Data_acquisition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web Crawler Import Setting"
      ],
      "metadata": {
        "id": "NTMa45nLjomK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ty1VEwPsgCiE"
      },
      "outputs": [],
      "source": [
        "# requests\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requesting by request"
      ],
      "metadata": {
        "id": "D2pF0P1tj-DS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL and response\n",
        "url = 'https://www.aihub.or.kr'\n",
        "response = requests.get(url)\n",
        "response.raise_for_status()"
      ],
      "metadata": {
        "id": "WE6As9Bkik6P"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HTML Parsing By BeautifulSoup"
      ],
      "metadata": {
        "id": "7JBkSQUthJT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BeautifulSoup Object & HTML Parsing\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "top3_section = soup.find('div', class_='secR')\n",
        "data_list = top3_section.find_all('div', class_='list')\n",
        "\n",
        "titles = []\n",
        "for data in data_list:\n",
        "  title = data.find('h3').get_text(strip=True)\n",
        "  titles.append(title.split(']')[-1].strip())\n",
        "\n",
        "for index, title in enumerate(titles, start=1):\n",
        "  print(f\"TOP {index}: {title}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJoetBcGiMsj",
        "outputId": "7c814b67-03d7-41bd-b827-fb37bcd99a3a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOP 1: 객체 간 관계성 인지용 한국형 비전 데이터\n",
            "TOP 2: 손∙팔 협조에 의한 파지-조작 동작 데이터\n",
            "TOP 3: 상용 자율주행차 야간 자동차 전용도로 데이터\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://ko.wikipedia.org/wiki/위키백과:대문'\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "  soup = BeautifulSoup(response.content,'html.parser')\n",
        "  title = soup.find('h1', id='firstHeading').text\n",
        "  print(f\"Title: {title}\")\n",
        "  first_paragraph = soup.find('p').text\n",
        "  print(f\"First paragraph: {first_paragraph}\")\n",
        "else:\n",
        "  print(f\"Filed to retrieve the web page. Status code: {response.status_code}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "as73_-j_iYA0",
        "outputId": "7515074f-ffaa-4c38-96ed-ec5003fcabd1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: 위키백과:대문\n",
            "First paragraph:  위키백과\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-rLHITRnCaY",
        "outputId": "80d976ea-b8c0-41b3-8d5e-1f788e3cc0a2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.22.0)\n",
            "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.25.1)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.6.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.7)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selenium: For web-scraping but time consuming and not a good idea"
      ],
      "metadata": {
        "id": "9g01rEmbn00n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import to use selenium"
      ],
      "metadata": {
        "id": "nz2aPlzjodUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By"
      ],
      "metadata": {
        "id": "EhKW6gJpni6B"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"https://ko.wikipedia.org/wiki/위키백과:대문\"\n",
        "\n",
        "# Chrome Options\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument(\"--headless\") # 브라우저 창을 띄우지 않음\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "\n",
        "# webdriver setting\n",
        "driver = webdriver.Chrome(options=options)\n",
        "\n",
        "try:\n",
        "  # opening wikipedia\n",
        "  driver.get(URL)\n",
        "  main_content = driver.find_element(By.CSS_SELECTOR, \"#mw-content-text > div.mw-content-ltr.mw-parser-output > div.main-box.main-top > div > p:nth-child(2)\").text\n",
        "  print(\"Main Content\", main_content)\n",
        "finally:\n",
        "  driver.quit()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUceLrxoocar",
        "outputId": "6e14b822-31df-4b3e-e638-a8c5df3191f0"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Main Content 우리 모두가 만들어가는 자유 백과사전\n",
            "문서 674,694개와 최근 기여자 1,814명\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scrapy: Webscraping\n"
      ],
      "metadata": {
        "id": "7FnpWsfJo00t"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RDIlXaovpl3y"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scrapy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wxUwb9ApqZV",
        "outputId": "1786f442-4786-45fe-c37d-b9e1fb9c6c98"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scrapy in /usr/local/lib/python3.10/dist-packages (2.11.2)\n",
            "Requirement already satisfied: Twisted>=18.9.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (24.3.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (42.0.8)\n",
            "Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.2.0)\n",
            "Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.3.1)\n",
            "Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.9.1)\n",
            "Requirement already satisfied: pyOpenSSL>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (24.1.0)\n",
            "Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.7.0)\n",
            "Requirement already satisfied: service-identity>=18.1.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (24.1.0)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (2.2.1)\n",
            "Requirement already satisfied: zope.interface>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (6.4.post2)\n",
            "Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.10/dist-packages (from scrapy) (0.3.1)\n",
            "Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (0.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from scrapy) (67.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scrapy) (24.1)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.10/dist-packages (from scrapy) (5.1.2)\n",
            "Requirement already satisfied: lxml>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (4.9.4)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (0.7.1)\n",
            "Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from scrapy) (2.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->scrapy) (1.16.0)\n",
            "Requirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.10/dist-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (23.2.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.6.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.0)\n",
            "Requirement already satisfied: automat>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.10/dist-packages (from Twisted>=18.9.0->scrapy) (23.10.4)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.10/dist-packages (from Twisted>=18.9.0->scrapy) (21.0.0)\n",
            "Requirement already satisfied: incremental>=22.10.0 in /usr/local/lib/python3.10/dist-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from Twisted>=18.9.0->scrapy) (4.12.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.7)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (2.31.0)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.15.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from automat>=0.8.0->Twisted>=18.9.0->scrapy) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2024.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scrapy START\n"
      ],
      "metadata": {
        "id": "v_fRzVOrplcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy startproject wikipedia_scraper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC9LsqVHpmCd",
        "outputId": "7d1536e0-d4e4-4313-eb96-2aad3c773285"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: scrapy.cfg already exists in /content/wikipedia_scraper\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scrapy import and Class"
      ],
      "metadata": {
        "id": "RZ5pWts4qIO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scrapy\n",
        "\n",
        "class WikipediaSpider(scrapy.Spider):\n",
        "  name = \"wikipedia\"\n",
        "  start_urls = [\n",
        "        'https://ko.wikipedia.org/wiki/위키백과:대문',\n",
        "    ]\n",
        "  def parse(self, response):\n",
        "        main_content = response.css('#mw-content-text > div.mw-content-ltr.mw-parser-output > div.main-pane > div.main-pane-right > div.wikipedia-ko.main-recommended.main-box').get()\n",
        "        yield {\n",
        "            'main_content': main_content,\n",
        "        }"
      ],
      "metadata": {
        "id": "uf-3oOOJqFR7"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a wikipedia_scraper/wikipedia_scraper/settings.py\n",
        "ROBOTSTXT_OBEY = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WruhPkllqFh1",
        "outputId": "bd64e647-7c7d-411e-f2d5-3d71cd496338"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to wikipedia_scraper/wikipedia_scraper/settings.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scrapy.crawler import CrawlerProcess\n",
        "# not working\n",
        "# process = CrawlerProcess({\n",
        "#     'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
        "#     'FEEDS': {\n",
        "#         'output.json': {\n",
        "#             'format': 'json',\n",
        "#             'encoding': 'utf8',\n",
        "#             'store_empty': False,\n",
        "#             'fields': None,\n",
        "#             'indent': 4,\n",
        "#         },\n",
        "#     },\n",
        "# })\n",
        "\n",
        "# process.crawl(WikipediaSpider)\n",
        "# process.start()"
      ],
      "metadata": {
        "id": "7iuU7IrQq4Th"
      },
      "execution_count": 62,
      "outputs": []
    }
  ]
}